{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3CHw7IYAFMT",
        "outputId": "47a8fa82-07a7-4e91-f913-fd1b885664e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s7BqbN6UAOL1"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qUoPky8BCPgW"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhrIw3nyRSkv"
      },
      "source": [
        "## **Model Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "15a0f76b05504eaa935d7f79b77b25c8",
            "24f2625ce6f64809904ec28261a183dd",
            "2f8f3ad43ae44e40852da9e37aec48c2",
            "78e997da6d5c42558adaeef030743045",
            "b965af4d7c86404db4dfaaf39ff4360d",
            "e715a6a8d715432f8f1b52eb055e4fe5",
            "c27cd12c69234f35a8a95f9c67337c84",
            "2a8994000fba412a92bd220bf3271798",
            "816fcee9f8bb4bb7b21c63a156a40752",
            "44b0f0b7acb744419e17bdff8ef0bc60",
            "77fe5067b1d74dddbfd1ee9f33379b00"
          ]
        },
        "id": "AR5_YlJNKPvD",
        "outputId": "6567cc90-6d53-4143-af4d-b17739119935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "No HF token found, proceeding without authentication (only works for public models)\n",
            "Quantization and PEFT libraries successfully imported\n",
            "Loading tokenizer for Equall/Saul-7B-Instruct-v1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model Equall/Saul-7B-Instruct-v1...\n",
            "Loading model with 4-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15a0f76b05504eaa935d7f79b77b25c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully\n",
            "Applying LoRA for parameter-efficient fine-tuning\n",
            "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes>=0.41.0 accelerate\n",
        "!pip install -q peft transformers datasets\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "MODEL_NAME = \"Equall/Saul-7B-Instruct-v1\"  # Legal specialized model\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/pfa_finetuning/gdpr/finetuned-saul-legal-model\"  # Local path to save model\n",
        "DATASET_PATH = \"/content/drive/MyDrive/pfa_finetuning/gdpr/articles/preprocessed/gdpr_QA_316.jsonl\"  # Local path to dataset\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 1  \n",
        "GRADIENT_ACCUMULATION_STEPS = 16  \n",
        "NUM_EPOCHS = 1\n",
        "MAX_SEQ_LEN = 768  \n",
        "SAVE_STEPS = 100\n",
        "FORMAT_TYPE = \"qa_format\"  #\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    print(\"HF token found, using for authentication\")\n",
        "except:\n",
        "    hf_token = None\n",
        "    print(\"No HF token found, proceeding without authentication (only works for public models)\")\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    from peft import (\n",
        "        LoraConfig,\n",
        "        get_peft_model,\n",
        "        prepare_model_for_kbit_training,\n",
        "        TaskType\n",
        "    )\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    QUANTIZATION_AVAILABLE = True\n",
        "    PEFT_AVAILABLE = True  # Explicitly set when imports succeed\n",
        "    print(\"Quantization and PEFT libraries successfully imported\")\n",
        "except ImportError:\n",
        "    QUANTIZATION_AVAILABLE = False\n",
        "    PEFT_AVAILABLE = False\n",
        "    print(\"Quantization not available - will load model in full precision (requires more memory)\")\n",
        "    try:\n",
        "        from peft import (\n",
        "            LoraConfig,\n",
        "            get_peft_model,\n",
        "            TaskType\n",
        "        )\n",
        "        PEFT_AVAILABLE = True\n",
        "        print(\"PEFT available without quantization\")\n",
        "    except ImportError:\n",
        "        print(\"PEFT not available - will fine-tune model without parameter-efficient methods\")\n",
        "\n",
        "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    token=hf_token,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=MAX_SEQ_LEN\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Loading model {MODEL_NAME}...\")\n",
        "model_loading_args = {\n",
        "    \"token\": hf_token,\n",
        "    \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    \"low_cpu_mem_usage\": True,\n",
        "}\n",
        "\n",
        "if QUANTIZATION_AVAILABLE and torch.cuda.is_available():\n",
        "    print(\"Loading model with 4-bit quantization...\")\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    model_loading_args[\"quantization_config\"] = quantization_config\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model_loading_args[\"device_map\"] = \"auto\"\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        **model_loading_args\n",
        "    )\n",
        "\n",
        "    if QUANTIZATION_AVAILABLE and torch.cuda.is_available():\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "    elif device.type != \"cpu\":\n",
        "        model = model.to(device)\n",
        "\n",
        "    print(\"Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Trying with less memory-intensive options...\")\n",
        "\n",
        "    # Try with lower precision and more memory optimizations\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            token=hf_token,\n",
        "            torch_dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "        if device.type != \"cpu\":\n",
        "            model = model.to(device)\n",
        "        print(\"Model loaded successfully with fallback options\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Still failed to load model: {e2}\")\n",
        "        print(\"Cannot proceed without loading model.\")\n",
        "        raise\n",
        "\n",
        "# Apply LoRA if PEFT is available\n",
        "if PEFT_AVAILABLE:\n",
        "    print(\"Applying LoRA for parameter-efficient fine-tuning\")\n",
        "    # Target modules for LoRA\n",
        "    if device.type == \"cpu\":\n",
        "        target_modules = [\"q_proj\", \"v_proj\"]  # Target fewer modules on CPU\n",
        "        LORA_R = 4  # Lower rank for CPU training\n",
        "    else:\n",
        "        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()  # Show trainable parameters\n",
        "else:\n",
        "    print(\"PEFT not available - fine-tuning full model (requires much more memory)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQh1pOatR7oc"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "3d1ab88724aa4c86964f814293440648",
            "d3b8377351b04965b74636511019538a",
            "2219a3cf3bb24f8c941c88c7463925c8",
            "994ad10f44864f61950aa6b9799db71e",
            "18309507ed344253911e7cc92973cf5b",
            "330ab2660ca6495281bfd345bc42c56f",
            "73a5ab7a10cd49888b96f809bf052303",
            "4edfc9a0cbf740608fb7e2c25261e156",
            "c97fbcb699874fab855b47c7182e98fb",
            "d5dd08a8d0c644b0b6c52589248e534d",
            "988c9569fc9d4ff18106c2ee35204ba3"
          ]
        },
        "id": "z5nrn10GMXoj",
        "outputId": "2f8427de-9af6-4d59-9b02-345bb54c3cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset with format: qa_format\n",
            "Using 'input' as question key and 'output' as answer key\n",
            "Dataset loaded with 316 examples\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d1ab88724aa4c86964f814293440648",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/316 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split dataset into 284 training and 32 test examples\n"
          ]
        }
      ],
      "source": [
        "# Sample dataset fallback\n",
        "def create_sample_dataset():\n",
        "    print(\"Creating a tiny sample dataset for testing purposes...\")\n",
        "    sample_data = [\n",
        "        {\"input\": \"What is the GDPR?\", \"output\": \"The GDPR is a regulation...\"},\n",
        "        {\"input\": \"Detail the specific obligations...\", \"output\": \"Under the CTR...\"}\n",
        "    ]\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    df[\"formatted_text\"] = df.apply(\n",
        "        lambda row: f\"<|user|>\\n{row['input']}\\n<|assistant|>\\n{row['output']}\",\n",
        "        axis=1\n",
        "    )\n",
        "    return Dataset.from_pandas(df[[\"formatted_text\"]])\n",
        "\n",
        "# Load and prepare dataset\n",
        "def load_legal_dataset(path, format_type):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Dataset file {path} not found!\")\n",
        "        return create_sample_dataset()\n",
        "\n",
        "    data = []\n",
        "    chunk_size = 100\n",
        "    if path.endswith('.jsonl'):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            chunk = []\n",
        "            for i, line in enumerate(f):\n",
        "                chunk.append(json.loads(line))\n",
        "                if (i + 1) % chunk_size == 0:\n",
        "                    data.extend(chunk)\n",
        "                    chunk = []\n",
        "                    gc.collect()\n",
        "            if chunk:\n",
        "                data.extend(chunk)\n",
        "    elif path.endswith('.json'):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    possible_question_keys = ['input', 'question', 'prompt', 'instruction']  # Prioritize 'input'\n",
        "    possible_answer_keys = ['output', 'answer', 'response']  # Prioritize 'output'\n",
        "    question_key = next((key for key in possible_question_keys if key in df.columns), None)\n",
        "    answer_key = next((key for key in possible_answer_keys if key in df.columns), None)\n",
        "\n",
        "    if format_type == \"instruction_input_output\":\n",
        "        df[\"formatted_text\"] = df.apply(\n",
        "            lambda row: f\"<|user|>\\n{row['instruction']}\\n\\n{row['input']}\\n<|assistant|>\\n{row['output']}\",\n",
        "            axis=1\n",
        "        )\n",
        "    elif format_type == \"text\":\n",
        "        df[\"formatted_text\"] = df[\"text\"]\n",
        "    elif format_type == \"discussion_text\":\n",
        "        df[\"formatted_text\"] = df[\"discussion_text\"].apply(\n",
        "            lambda text: text.replace(\"[INST]\", \"<|user|>\\n\").replace(\"[/INST]\", \"\\n<|assistant|>\\n\")\n",
        "        )\n",
        "    elif format_type == \"qa_format\":\n",
        "        if question_key and answer_key:\n",
        "            print(f\"Using '{question_key}' as question key and '{answer_key}' as answer key\")\n",
        "            df[\"formatted_text\"] = df.apply(\n",
        "                lambda row: f\"<|user|>\\n{row[question_key]}\\n<|assistant|>\\n{row[answer_key]}\",\n",
        "                axis=1\n",
        "            )\n",
        "        else:\n",
        "            raise KeyError(f\"Could not find suitable question/answer keys in dataset. Found columns: {list(df.columns)}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown format type: {format_type}\")\n",
        "\n",
        "    return Dataset.from_pandas(df[[\"formatted_text\"]])\n",
        "\n",
        "# Load dataset\n",
        "print(f\"Loading dataset with format: {FORMAT_TYPE}\")\n",
        "dataset = load_legal_dataset(DATASET_PATH, FORMAT_TYPE)\n",
        "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"formatted_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=[\"formatted_text\"],\n",
        "    desc=\"Tokenizing dataset\",\n",
        ")\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"Split dataset into {len(tokenized_dataset['train'])} training and {len(tokenized_dataset['test'])} test examples\")\n",
        "\n",
        "del dataset\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYhFp2ZlSED6"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ixSeNgcnMeeD",
        "outputId": "a42359a9-b410-4e6d-f86d-cedefdb45623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient checkpointing enabled for memory efficiency\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [34/34 29:54, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.327400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.233300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.140900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.119100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.114800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.992000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.079800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.071700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.972600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.067500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.058400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.075700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.975100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2 - Training Loss: 1.3245\n",
            "GPU Memory: 4.42 GB\n",
            "Step 4 - Training Loss: 1.3274\n",
            "GPU Memory: 4.42 GB\n",
            "Step 6 - Training Loss: 1.2333\n",
            "GPU Memory: 4.42 GB\n",
            "Step 8 - Training Loss: 1.1409\n",
            "GPU Memory: 4.42 GB\n",
            "Step 10 - Training Loss: 1.1191\n",
            "GPU Memory: 4.42 GB\n",
            "Step 12 - Training Loss: 1.1148\n",
            "GPU Memory: 4.42 GB\n",
            "Step 14 - Training Loss: 1.1403\n",
            "GPU Memory: 4.42 GB\n",
            "Step 16 - Training Loss: 0.9920\n",
            "GPU Memory: 4.42 GB\n",
            "Step 18 - Training Loss: 1.0798\n",
            "GPU Memory: 4.42 GB\n",
            "Step 20 - Training Loss: 1.0717\n",
            "GPU Memory: 4.42 GB\n",
            "Step 22 - Training Loss: 0.9726\n",
            "GPU Memory: 4.42 GB\n",
            "Step 24 - Training Loss: 1.0675\n",
            "GPU Memory: 4.42 GB\n",
            "Step 26 - Training Loss: 1.0584\n",
            "GPU Memory: 4.42 GB\n",
            "Step 28 - Training Loss: 1.0808\n",
            "GPU Memory: 4.42 GB\n",
            "Step 30 - Training Loss: 1.0757\n",
            "GPU Memory: 4.42 GB\n",
            "Step 32 - Training Loss: 1.0352\n",
            "GPU Memory: 4.42 GB\n",
            "Step 34 - Training Loss: 0.9751\n",
            "GPU Memory: 4.42 GB\n",
            "Saving model...\n",
            "Model saved to /content/drive/MyDrive/pfa_finetuning/gdpr/finetuned-saul-legal-model/final_model\n"
          ]
        }
      ],
      "source": [
        "# Custom callback\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and \"loss\" in logs:\n",
        "            print(f\"Step {state.global_step} - Training Loss: {logs['loss']:.4f}\")\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "                print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "        if logs and \"eval_loss\" in logs:\n",
        "            print(f\"Step {state.global_step} - Eval Loss: {logs['eval_loss']:.4f}\")\n",
        "\n",
        "# Gradient checkpointing\n",
        "gradient_checkpointing = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    gradient_checkpointing = True\n",
        "    print(\"Gradient checkpointing enabled for memory efficiency\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    optim=\"adamw_torch\",\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    report_to=\"tensorboard\",\n",
        "    disable_tqdm=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=True if torch.cuda.is_available() else False,\n",
        "    ddp_find_unused_parameters=False,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[ProgressCallback()],\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    training_successful = True\n",
        "except Exception as e:\n",
        "    training_successful = False\n",
        "    print(f\"Training error: {e}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Current GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "        print(f\"Max GPU Memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(\"\\nTrying to reduce model size or batch size might help.\")\n",
        "\n",
        "# Save model\n",
        "if training_successful:\n",
        "    print(\"Saving model...\")\n",
        "    if PEFT_AVAILABLE:\n",
        "        model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
        "    else:\n",
        "        model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
        "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
        "    print(f\"Model saved to {os.path.join(OUTPUT_DIR, 'final_model')}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-S71iG0SXzH"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z36FRt_oZAn7",
        "outputId": "0d5ce6b0-c22f-4950-ef79-00eb8f8f38f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing model with prompt:\n",
            "<|user|>\n",
            "Based on the following section, does it comply with GDPR Article 65(1)(a)? Respond with 'Compliant' or 'Not Compliant' and briefly explain why.\n",
            "\n",
            "Section:\n",
            "No data processing agreement has been concluded with the company whose servers contained the resources of the Public Information Bulletin (BIP) of the Municipal Office in Aleksandrów Kujawski. For this reason, a fine of 40.000 PLN (9400 EUR) was imposed on the mayor of the city.\n",
            "<|assistant|>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response: **Compliance with GDPR Article 65(1)(a):**\n",
            "- **Legal Basis:** GDPR Article 65(1)(a) states that Member States may specify the tasks and powers of the data protection authority.\n",
            "- **Compliance Explanation:** The provided section outlines a situation where a data processing agreement has not been concluded with a company whose servers contained resources of a Public Information Bulletin (BIP). This action led to a fine being imposed on the mayor of the city. This scenario is in line with GDPR Article 65(1)(a) as it demonstrates the tasks and powers of the data protection authority, specifically enforcing compliance with GDPR data processing agreements.\n",
            "\n",
            "**Relevant GDPR Article:** Article 65(1)(a) - Tasks and powers of the supervisory authority.\n",
            "\n",
            "**Source of Compliance:** GDPR Article 65(1)(a) - Legal Basis for Data Protection Authority Tasks and Powers.\n",
            "\n",
            "**Assessment:** The provided section demonstrates how a data protection authority enforces GDPR compliance by imposing a fine for non-compliance with data processing\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Instruction and section to test\n",
        "instruction = \"Based on the following section, does it comply with GDPR Article 65(1)(a)? Respond with 'Compliant' or 'Not Compliant' and briefly explain why.\"\n",
        "test_section = \"No data processing agreement has been concluded with the company whose servers contained the resources of the Public Information Bulletin (BIP) of the Municipal Office in Aleksandrów Kujawski. For this reason, a fine of 40.000 PLN (9400 EUR) was imposed on the mayor of the city.\"\n",
        "\n",
        "# Construct prompt\n",
        "test_prompt = f\"<|user|>\\n{instruction}\\n\\nSection:\\n{test_section}\\n<|assistant|>\"\n",
        "print(f\"\\nTesting model with prompt:\\n{test_prompt}\")\n",
        "\n",
        "# Tokenize and prepare inputs\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate response\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode and extract response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"<|assistant|>\")[1].strip() if \"<|assistant|>\" in response else response\n",
        "    print(f\"\\nResponse: {response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error generating test response: {e}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15a0f76b05504eaa935d7f79b77b25c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24f2625ce6f64809904ec28261a183dd",
              "IPY_MODEL_2f8f3ad43ae44e40852da9e37aec48c2",
              "IPY_MODEL_78e997da6d5c42558adaeef030743045"
            ],
            "layout": "IPY_MODEL_b965af4d7c86404db4dfaaf39ff4360d"
          }
        },
        "18309507ed344253911e7cc92973cf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2219a3cf3bb24f8c941c88c7463925c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edfc9a0cbf740608fb7e2c25261e156",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c97fbcb699874fab855b47c7182e98fb",
            "value": 316
          }
        },
        "24f2625ce6f64809904ec28261a183dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e715a6a8d715432f8f1b52eb055e4fe5",
            "placeholder": "​",
            "style": "IPY_MODEL_c27cd12c69234f35a8a95f9c67337c84",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2a8994000fba412a92bd220bf3271798": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8f3ad43ae44e40852da9e37aec48c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8994000fba412a92bd220bf3271798",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_816fcee9f8bb4bb7b21c63a156a40752",
            "value": 6
          }
        },
        "330ab2660ca6495281bfd345bc42c56f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d1ab88724aa4c86964f814293440648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3b8377351b04965b74636511019538a",
              "IPY_MODEL_2219a3cf3bb24f8c941c88c7463925c8",
              "IPY_MODEL_994ad10f44864f61950aa6b9799db71e"
            ],
            "layout": "IPY_MODEL_18309507ed344253911e7cc92973cf5b"
          }
        },
        "44b0f0b7acb744419e17bdff8ef0bc60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edfc9a0cbf740608fb7e2c25261e156": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a5ab7a10cd49888b96f809bf052303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77fe5067b1d74dddbfd1ee9f33379b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78e997da6d5c42558adaeef030743045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b0f0b7acb744419e17bdff8ef0bc60",
            "placeholder": "​",
            "style": "IPY_MODEL_77fe5067b1d74dddbfd1ee9f33379b00",
            "value": " 6/6 [02:25&lt;00:00, 23.66s/it]"
          }
        },
        "816fcee9f8bb4bb7b21c63a156a40752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "988c9569fc9d4ff18106c2ee35204ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "994ad10f44864f61950aa6b9799db71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5dd08a8d0c644b0b6c52589248e534d",
            "placeholder": "​",
            "style": "IPY_MODEL_988c9569fc9d4ff18106c2ee35204ba3",
            "value": " 316/316 [00:00&lt;00:00, 424.97 examples/s]"
          }
        },
        "b965af4d7c86404db4dfaaf39ff4360d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27cd12c69234f35a8a95f9c67337c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c97fbcb699874fab855b47c7182e98fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3b8377351b04965b74636511019538a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330ab2660ca6495281bfd345bc42c56f",
            "placeholder": "​",
            "style": "IPY_MODEL_73a5ab7a10cd49888b96f809bf052303",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "d5dd08a8d0c644b0b6c52589248e534d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e715a6a8d715432f8f1b52eb055e4fe5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
